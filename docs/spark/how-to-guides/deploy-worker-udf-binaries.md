---
title: Distribuire .NET per Apache Spark Worker e i file binari delle funzioni definite dall'utente
description: Informazioni su come distribuire .NET per Apache Spark di lavoro e binari di funzioni definite dall'utente.
ms.date: 06/25/2020
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: 672a32c430bd702167a294d2b895ac1ac90bf67e
ms.sourcegitcommit: e02d17b2cf9c1258dadda4810a5e6072a0089aee
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 07/01/2020
ms.locfileid: "85617717"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="6fecf-103">Distribuire .NET per Apache Spark Worker e i file binari delle funzioni definite dall'utente</span><span class="sxs-lookup"><span data-stu-id="6fecf-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="6fecf-104">In questa procedura vengono fornite istruzioni generali su come distribuire .NET per i file binari di Apache Spark Worker e di funzioni definite dall'utente.</span><span class="sxs-lookup"><span data-stu-id="6fecf-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="6fecf-105">Vengono fornite informazioni sulle variabili di ambiente da configurare, oltre ad alcuni parametri di uso comune per l'avvio di applicazioni con `spark-submit` .</span><span class="sxs-lookup"><span data-stu-id="6fecf-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

[!INCLUDE [spark-preview-note](../../../includes/spark-preview-note.md)]

## <a name="configurations"></a><span data-ttu-id="6fecf-106">Configurazioni</span><span class="sxs-lookup"><span data-stu-id="6fecf-106">Configurations</span></span>
<span data-ttu-id="6fecf-107">Nelle configurazioni sono illustrate le impostazioni generali per le variabili di ambiente e i parametri per distribuire .NET per i file binari di Apache Spark Worker e di funzioni definite dall'utente.</span><span class="sxs-lookup"><span data-stu-id="6fecf-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="6fecf-108">Variabili di ambiente</span><span class="sxs-lookup"><span data-stu-id="6fecf-108">Environment variables</span></span>
<span data-ttu-id="6fecf-109">Quando si distribuiscono i ruoli di lavoro e si scrivono UDF, è possibile che sia necessario impostare alcune variabili di ambiente di uso comune:</span><span class="sxs-lookup"><span data-stu-id="6fecf-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span>

| <span data-ttu-id="6fecf-110">Variabile di ambiente</span><span class="sxs-lookup"><span data-stu-id="6fecf-110">Environment Variable</span></span>         | <span data-ttu-id="6fecf-111">Descrizione</span><span class="sxs-lookup"><span data-stu-id="6fecf-111">Description</span></span>
| :--------------------------- | :----------
| <span data-ttu-id="6fecf-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="6fecf-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="6fecf-113">Percorso in cui <code>Microsoft.Spark.Worker</code> è stato generato il file binario.</span><span class="sxs-lookup"><span data-stu-id="6fecf-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="6fecf-114">Viene usato dal driver Spark e verrà passato agli esecutori Spark.</span><span class="sxs-lookup"><span data-stu-id="6fecf-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="6fecf-115">Se questa variabile non è impostata, gli esecutori Spark eseguiranno la ricerca nel percorso specificato nella <code>PATH</code> variabile di ambiente.</span><span class="sxs-lookup"><span data-stu-id="6fecf-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="6fecf-116">_ad esempio, "C:\bin\Microsoft.Spark.Worker"_</span><span class="sxs-lookup"><span data-stu-id="6fecf-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="6fecf-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="6fecf-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="6fecf-118">Percorsi delimitati da virgole in cui <code>Microsoft.Spark.Worker</code> vengono caricati gli assembly.</span><span class="sxs-lookup"><span data-stu-id="6fecf-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="6fecf-119">Si noti che se un percorso inizia con ".", la directory di lavoro verrà anteposta.</span><span class="sxs-lookup"><span data-stu-id="6fecf-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="6fecf-120">Se è in **modalità Yarn**, "." rappresenterà la directory di lavoro del contenitore.</span><span class="sxs-lookup"><span data-stu-id="6fecf-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="6fecf-121">_ad esempio, "C:\Users \\ &lt; nome utente &gt; \\ &lt; mysparkapp &gt; \bin\Debug \\ &lt; DotNet versione &gt; "_</span><span class="sxs-lookup"><span data-stu-id="6fecf-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="6fecf-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="6fecf-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="6fecf-123">Se si vuole <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">eseguire il debug di una funzione definita dall'utente</a>, impostare questa variabile di ambiente su prima di <code>1</code> eseguire <code>spark-submit</code> .</span><span class="sxs-lookup"><span data-stu-id="6fecf-123">If you want to <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="6fecf-124">Opzioni dei parametri:</span><span class="sxs-lookup"><span data-stu-id="6fecf-124">Parameter options</span></span>
<span data-ttu-id="6fecf-125">Quando l'applicazione Spark è in [bundle](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), è possibile avviarla usando `spark-submit` .</span><span class="sxs-lookup"><span data-stu-id="6fecf-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="6fecf-126">Nella tabella seguente vengono illustrate alcune delle opzioni di uso comune:</span><span class="sxs-lookup"><span data-stu-id="6fecf-126">The following table shows some of the commonly used options:</span></span>

| <span data-ttu-id="6fecf-127">Nome parametro</span><span class="sxs-lookup"><span data-stu-id="6fecf-127">Parameter Name</span></span>        | <span data-ttu-id="6fecf-128">Descrizione</span><span class="sxs-lookup"><span data-stu-id="6fecf-128">Description</span></span>
| :---------------------| :----------
| <span data-ttu-id="6fecf-129">--Class</span><span class="sxs-lookup"><span data-stu-id="6fecf-129">--class</span></span>               | <span data-ttu-id="6fecf-130">Il punto di ingresso per l'applicazione.</span><span class="sxs-lookup"><span data-stu-id="6fecf-130">The entry point for your application.</span></span></br><span data-ttu-id="6fecf-131">_ad esempio, org. Apache. Spark. deploy. dotnet. DotnetRunner_</span><span class="sxs-lookup"><span data-stu-id="6fecf-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="6fecf-132">--Master</span><span class="sxs-lookup"><span data-stu-id="6fecf-132">--master</span></span>              | <span data-ttu-id="6fecf-133"><a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">URL Master</a> per il cluster.</span><span class="sxs-lookup"><span data-stu-id="6fecf-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="6fecf-134">_ad esempio Yarn_</span><span class="sxs-lookup"><span data-stu-id="6fecf-134">_e.g. yarn_</span></span>
| <span data-ttu-id="6fecf-135">--Deploy-Mode</span><span class="sxs-lookup"><span data-stu-id="6fecf-135">--deploy-mode</span></span>         | <span data-ttu-id="6fecf-136">Indica se distribuire il driver nei nodi di lavoro ( <code>cluster</code> ) o localmente come client esterno ( <code>client</code> ).</span><span class="sxs-lookup"><span data-stu-id="6fecf-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="6fecf-137">Valore predefinito: <code>client</code></span><span class="sxs-lookup"><span data-stu-id="6fecf-137">Default: <code>client</code></span></span>
| <span data-ttu-id="6fecf-138">--conf</span><span class="sxs-lookup"><span data-stu-id="6fecf-138">--conf</span></span>                | <span data-ttu-id="6fecf-139">Proprietà di configurazione di Spark arbitraria nel <code>key=value</code> formato.</span><span class="sxs-lookup"><span data-stu-id="6fecf-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="6fecf-140">_ad esempio Spark. Yarn. appMasterEnv. DOTNET_WORKER_DIR = .\worker\Microsoft.Spark.Worker_</span><span class="sxs-lookup"><span data-stu-id="6fecf-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="6fecf-141">--file</span><span class="sxs-lookup"><span data-stu-id="6fecf-141">--files</span></span>               | <span data-ttu-id="6fecf-142">Elenco di file delimitati da virgole da inserire nella directory di lavoro di ogni Executor.</span><span class="sxs-lookup"><span data-stu-id="6fecf-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="6fecf-143">Si noti che questa opzione è applicabile solo per la modalità Yarn.</span><span class="sxs-lookup"><span data-stu-id="6fecf-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="6fecf-144">Supporta la specifica di nomi file con # simile a Hadoop.</span><span class="sxs-lookup"><span data-stu-id="6fecf-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="6fecf-145">_ad esempio <code>myLocalSparkApp.dll#appSeen.dll</code> , L'applicazione deve usare il nome come <code>appSeen.dll</code> riferimento <code>myLocalSparkApp.dll</code> quando viene eseguito su Yarn._</span><span class="sxs-lookup"><span data-stu-id="6fecf-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="6fecf-146">--archivi</span><span class="sxs-lookup"><span data-stu-id="6fecf-146">--archives</span></span>          | <span data-ttu-id="6fecf-147">Elenco delimitato da virgole di archivi da estrarre nella directory di lavoro di ogni Executor.</span><span class="sxs-lookup"><span data-stu-id="6fecf-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="6fecf-148">Si noti che questa opzione è applicabile solo per la modalità Yarn.</span><span class="sxs-lookup"><span data-stu-id="6fecf-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="6fecf-149">Supporta la specifica di nomi file con # simile a Hadoop.</span><span class="sxs-lookup"><span data-stu-id="6fecf-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="6fecf-150">_ad esempio <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code> , Il file zip verrà copiato e estratto nella <code>worker</code> cartella._</span><span class="sxs-lookup"><span data-stu-id="6fecf-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="6fecf-151">applicazione-jar</span><span class="sxs-lookup"><span data-stu-id="6fecf-151">application-jar</span></span>       | <span data-ttu-id="6fecf-152">Percorso di un file jar in bundle che include l'applicazione e tutte le dipendenze.</span><span class="sxs-lookup"><span data-stu-id="6fecf-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="6fecf-153">_ad esempio hdfs:// &lt; percorso del file jar &gt; /Microsoft-Spark- &lt; version &gt; . jar_</span><span class="sxs-lookup"><span data-stu-id="6fecf-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="6fecf-154">argomenti dell'applicazione</span><span class="sxs-lookup"><span data-stu-id="6fecf-154">application-arguments</span></span> | <span data-ttu-id="6fecf-155">Argomenti passati al metodo principale della classe principale, se presenti.</span><span class="sxs-lookup"><span data-stu-id="6fecf-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="6fecf-156">_ad esempio, HDFS://il percorso dell'app &lt; &gt; / &lt; &gt; . zip app &lt; nome app. &gt; &lt;&gt;_</span><span class="sxs-lookup"><span data-stu-id="6fecf-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="6fecf-157">Specificare tutti gli elementi `--options` prima dell' `application-jar` avvio delle applicazioni con `spark-submit` . in caso contrario, verranno ignorati.</span><span class="sxs-lookup"><span data-stu-id="6fecf-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="6fecf-158">Per altre informazioni, vedere [ `spark-submit` Opzioni](https://spark.apache.org/docs/latest/submitting-applications.html) ed [esecuzione di Spark su Yarn dettagli](https://spark.apache.org/docs/latest/running-on-yarn.html).</span><span class="sxs-lookup"><span data-stu-id="6fecf-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="6fecf-159">Domande frequenti</span><span class="sxs-lookup"><span data-stu-id="6fecf-159">Frequently asked questions</span></span>
### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="6fecf-160">Quando si esegue un'app Spark con funzioni definite dall'utente, viene generato un errore ' FileNotFoundException '.</span><span class="sxs-lookup"><span data-stu-id="6fecf-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="6fecf-161">Cosa devo fare?</span><span class="sxs-lookup"><span data-stu-id="6fecf-161">What should I do?</span></span>
> <span data-ttu-id="6fecf-162">**Errore:** [errore] [TaskRunner] [0] ProcessStream () non riuscito con eccezione: System. io. FileNotFoundException: assembly ' MySparkApp, Version = 1.0.0.0, Culture = neutral, PublicKeyToken = null ' file non trovato:' mySparkApp.dll'</span><span class="sxs-lookup"><span data-stu-id="6fecf-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="6fecf-163">**Risposta:** Verificare che la `DOTNET_ASSEMBLY_SEARCH_PATHS` variabile di ambiente sia impostata correttamente.</span><span class="sxs-lookup"><span data-stu-id="6fecf-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="6fecf-164">Deve corrispondere al percorso contenente il `mySparkApp.dll` .</span><span class="sxs-lookup"><span data-stu-id="6fecf-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="6fecf-165">Dopo l'aggiornamento di .NET per Apache Spark versione e la reimpostazione della `DOTNET_WORKER_DIR` variabile di ambiente, perché viene comunque ottenuto l' `IOException` errore seguente?</span><span class="sxs-lookup"><span data-stu-id="6fecf-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>
> <span data-ttu-id="6fecf-166">**Errore:** Attività persa 0,0 nella fase 11,0 (TID 24, localhost, driver Executor): Java. io. IOException: non è possibile eseguire il programma "Microsoft.Spark.Worker.exe": errore CreateProcess = 2, il sistema non riesce a trovare il file specificato.</span><span class="sxs-lookup"><span data-stu-id="6fecf-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="6fecf-167">**Risposta:** Provare a riavviare prima la finestra di PowerShell (o altre finestre di comando) in modo da poter usare i valori delle variabili di ambiente più recenti.</span><span class="sxs-lookup"><span data-stu-id="6fecf-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="6fecf-168">Avviare quindi il programma.</span><span class="sxs-lookup"><span data-stu-id="6fecf-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="6fecf-169">Dopo aver inviato l'applicazione Spark, viene generato l'errore `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'` .</span><span class="sxs-lookup"><span data-stu-id="6fecf-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>
> <span data-ttu-id="6fecf-170">**Errore:** [errore] [TaskRunner] [0] ProcessStream () non riuscito con eccezione: System. TypeLoadException: Impossibile caricare il tipo ' System. Runtime. Remoting. Contexts. Context ' dall'assembly ' mscorlib, Version = 4.0.0.0, Culture = neutral, PublicKeyToken =.. .'.</span><span class="sxs-lookup"><span data-stu-id="6fecf-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="6fecf-171">**Risposta:** Controllare la `Microsoft.Spark.Worker` versione in uso.</span><span class="sxs-lookup"><span data-stu-id="6fecf-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="6fecf-172">Sono disponibili due versioni: **.NET Framework 4.6.1** e **.NET Core 2.1. x**.</span><span class="sxs-lookup"><span data-stu-id="6fecf-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 2.1.x**.</span></span> <span data-ttu-id="6fecf-173">In questo caso, `Microsoft.Spark.Worker.net461.win-x64-<version>` è consigliabile usare (che è possibile [scaricare](https://github.com/dotnet/spark/releases)) perché `System.Runtime.Remoting.Contexts.Context` è solo per .NET Framework.</span><span class="sxs-lookup"><span data-stu-id="6fecf-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="6fecf-174">Ricerca per categorie eseguire l'applicazione Spark con funzioni definite dall'utente in YARN?</span><span class="sxs-lookup"><span data-stu-id="6fecf-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="6fecf-175">Quali variabili e parametri di ambiente è opportuno usare?</span><span class="sxs-lookup"><span data-stu-id="6fecf-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="6fecf-176">**Risposta:** Per avviare l'applicazione Spark in YARN, le variabili di ambiente devono essere specificate come `spark.yarn.appMasterEnv.[EnvironmentVariableName]` .</span><span class="sxs-lookup"><span data-stu-id="6fecf-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="6fecf-177">Vedere di seguito come esempio per l'uso di `spark-submit` :</span><span class="sxs-lookup"><span data-stu-id="6fecf-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-2.4.x-<version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="6fecf-178">Passaggi successivi</span><span class="sxs-lookup"><span data-stu-id="6fecf-178">Next steps</span></span>

* [<span data-ttu-id="6fecf-179">Introduzione a .NET per Apache Spark</span><span class="sxs-lookup"><span data-stu-id="6fecf-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="6fecf-180">Eseguire il debug di un'applicazione .NET per Apache Spark in Windows</span><span class="sxs-lookup"><span data-stu-id="6fecf-180">Debug a .NET for Apache Spark application on Windows</span></span>](debug.md)
